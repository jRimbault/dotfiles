#!/usr/bin/env -S uv run --script
#
# /// script
# requires-python = ">=3.12"
# dependencies = ["httpx"]
# ///
"""Get the visible repositories for a given GitHub user or organization"""

import argparse
import asyncio
import functools
import json
import os
import re
import sys
import time
from datetime import timedelta
from pathlib import Path
from typing import NamedTuple

import httpx

CACHE_TTL = timedelta(minutes=10)
GITHUB_API = "https://api.github.com"


class RateLimited(SystemExit):
    """Raised when the GitHub API rate limit is exceeded."""

    def __init__(self, resp):
        reset = resp.headers.get("x-ratelimit-reset")
        if reset:
            wait = int(int(reset) - time.time()) // 60
            msg = f"github: rate limit exceeded, resets in {wait}min"
        else:
            msg = "github: rate limit exceeded"
        super().__init__(msg)


def check_rate_limit(resp):
    """Raise RateLimited if the response indicates rate limit exhaustion."""
    if resp.status_code == 403 and resp.headers.get("x-ratelimit-remaining") == "0":
        raise RateLimited(resp)


class RepoPage(NamedTuple):
    """A single page of repos as returned by the GitHub API."""

    repos: list[dict]
    etag: str | None


class Repository(NamedTuple):
    name: str
    description: str | None

    def __repr__(self):
        if self.description is None:
            return self.name
        return f"{self.name:<30} {self.description[0:89]}"


class NotModified(Exception):
    """Raised by fetch generators when the server returns HTTP 304."""


class PublicArgs(NamedTuple):
    """Unauthenticated mode — REST API, public repos only."""

    user: str
    no_cache: bool = False


class AuthenticatedArgs(NamedTuple):
    """Authenticated mode — GraphQL API, all visible repos."""

    user: str
    token: str


Args = PublicArgs | AuthenticatedArgs


async def main(args: Args):
    match args:
        case AuthenticatedArgs():
            source = get_all_repositories_graphql(args)
        case PublicArgs():
            source = get_all_repositories_rest(args)
    async for repo in source:
        print(repo, flush=True)


# -- REST API (unauthenticated, public repos only) --


async def get_all_repositories_rest(args: PublicArgs):
    """Fetch public repos via REST API with caching and connection reuse."""

    def parse_next_link(link_header):
        """Extract the 'next' page URL from a GitHub Link header."""
        match = re.search(r'<([^>]+)>;\s*rel="next"', link_header)
        return match.group(1) if match else None

    @github_cache
    async def fetch_repos(client, user, *, etag=None):
        """Yield RepoPage per page, raise NotModified for 304."""

        async def fetch_org_repos(client, org):
            """Fallback for organizations that don't resolve via /users/."""
            url = f"{GITHUB_API}/orgs/{org}/repos?per_page=100"
            while url:
                resp = await client.get(url)
                check_rate_limit(resp)
                if resp.status_code != 200:
                    raise RuntimeError(f"GitHub API error: {resp.status_code}")
                yield RepoPage(resp.json(), resp.headers.get("etag"))
                url = parse_next_link(resp.headers.get("link", ""))

        headers = {"if-none-match": etag} if etag else {}
        resp = await client.get(
            f"{GITHUB_API}/users/{user}/repos?per_page=100", headers=headers
        )

        if resp.status_code == 304:
            raise NotModified
        if resp.status_code == 404:
            # /users/ endpoint 404s for some orgs, try /orgs/ directly
            async for page in fetch_org_repos(client, user):
                yield page
            return
        check_rate_limit(resp)
        if resp.status_code != 200:
            raise RuntimeError(f"GitHub API error: {resp.status_code}")

        yield RepoPage(resp.json(), resp.headers.get("etag"))
        next_url = parse_next_link(resp.headers.get("link", ""))

        while next_url:
            resp = await client.get(next_url)
            check_rate_limit(resp)
            if resp.status_code != 200:
                raise RuntimeError(
                    f"GitHub API error during pagination: {resp.status_code}"
                )
            yield RepoPage(resp.json(), resp.headers.get("etag"))
            next_url = parse_next_link(resp.headers.get("link", ""))

    async with httpx.AsyncClient(
        headers={
            "Accept": "application/vnd.github+json",
            "User-Agent": "github-repos-cli",
        }
    ) as client:
        async for repo in fetch_repos(client, args.user, no_cache=args.no_cache):
            yield Repository(repo["name"], repo.get("description"))


def github_cache(fn):
    """Decorator for async generators that yield RepoPage per page.

    Streams individual repo dicts through to the caller while accumulating
    them for the disk cache. Handles ETag revalidation transparently:
      - Fresh cache: yields cached repos, no network
      - 304 Not Modified: refreshes timestamp, yields cached repos
      - New data: streams pages through, saves to cache at end
    """

    class RepoCache:
        """Manages a per-user JSON cache file under ~/.cache/github-repos/."""

        def __init__(self, user):
            self._path = Path.home() / ".cache" / "github-repos" / f"{user}.json"
            self._data = None

        def _read(self):
            if self._data is None:
                try:
                    self._data = json.loads(self._path.read_text())
                except (FileNotFoundError, json.JSONDecodeError):
                    self._data = {}
            return self._data

        def exists(self):
            return bool(self._read().get("repos"))

        @property
        def etag(self):
            return self._read().get("etag")

        def is_fresh(self):
            """True if the cache exists and is younger than CACHE_TTL."""
            data = self._read()
            timestamp = data.get("timestamp", 0)
            age = timedelta(seconds=time.time() - timestamp)
            return bool(data.get("repos")) and age < CACHE_TTL

        def load(self):
            """Return the raw repo dicts from cache."""
            return self._read().get("repos", [])

        def touch(self):
            """Refresh the cache timestamp without changing the data."""
            data = self._read()
            data["timestamp"] = time.time()
            self._write(data)

        def save(self, repos, etag):
            """Write fetched repos and ETag to disk."""
            data = {
                "etag": etag,
                "timestamp": time.time(),
                "repos": [
                    {"name": r["name"], "description": r.get("description")}
                    for r in repos
                ],
            }
            self._write(data)

        def _write(self, data):
            self._path.parent.mkdir(parents=True, exist_ok=True)
            self._path.write_text(json.dumps(data))
            self._data = data

    @functools.wraps(fn)
    async def wrapper(client, user, *, no_cache=False):
        cache = RepoCache(user)
        if not no_cache and cache.is_fresh():
            for repo in cache.load():
                yield repo
            return

        etag = cache.etag if not no_cache and cache.exists() else None
        accumulated = []
        first_etag = None

        try:
            async for page in fn(client, user, etag=etag):
                if first_etag is None:
                    first_etag = page.etag
                accumulated.extend(page.repos)
                for repo in page.repos:
                    yield repo
        except NotModified:
            cache.touch()
            for repo in cache.load():
                yield repo
            return
        except RateLimited:
            # Serve stale cache if available, otherwise propagate
            if cache.exists():
                print(
                    "github: rate limited, serving stale cache",
                    file=sys.stderr,
                )
                for repo in cache.load():
                    yield repo
                return
            raise

        cache.save(accumulated, first_etag)

    return wrapper


# -- GraphQL API (requires authentication) --


GRAPHQL_QUERY_DATA = """{
  repositories(ownerAffiliations: OWNER, first: 100, after: $cursor) {
   nodes {
    name
    description
   }
   pageInfo {
    hasNextPage
    endCursor
   }
  }
}"""
GRAPHQL_QUERY = """query ($user: String!, $cursor: String) {
 user(login: $user) %s
 organization(login: $user) %s
}""" % (
    GRAPHQL_QUERY_DATA,
    GRAPHQL_QUERY_DATA,
)


async def get_all_repositories_graphql(args: AuthenticatedArgs):
    """Fetch all visible repos via GraphQL API (authenticated)."""
    async with httpx.AsyncClient(
        headers={
            "Content-Type": "application/json; charset=utf-8",
            "Authorization": f"token {args.token}",
        }
    ) as client:
        cursor = None
        has_next = True
        while has_next:
            variables = {"user": args.user}
            if cursor:
                variables["cursor"] = cursor
            resp = await client.post(
                f"{GITHUB_API}/graphql",
                json={"query": GRAPHQL_QUERY, "variables": variables},
            )
            check_rate_limit(resp)
            graph = GraphResponse(resp.json())
            for repo in graph.repos():
                yield repo
            has_next = graph.has_next_page
            cursor = graph.end_cursor


class GraphResponse:
    def __init__(self, data):
        owner = data["data"].get("user") or data["data"].get("organization")
        self.repositories = owner["repositories"]
        self.has_next_page = self.repositories["pageInfo"]["hasNextPage"]
        self.end_cursor = self.repositories["pageInfo"]["endCursor"]

    def repos(self):
        for node in self.repositories["nodes"]:
            yield Repository(node["name"], node.get("description"))


def parse_args(argv=sys.argv[1:]) -> Args:
    p = argparse.ArgumentParser(description=__doc__)
    p.add_argument("user", nargs="?", default=os.environ.get("USER"))
    p.add_argument(
        "--token",
        default=os.environ.get("GITHUB_API_TOKEN"),
        help="GitHub API token (default: $GITHUB_API_TOKEN)",
    )
    p.add_argument(
        "--no-cache",
        action="store_true",
        help="bypass the disk cache and fetch fresh data",
    )
    ns = p.parse_args(argv)
    if ns.token:
        return AuthenticatedArgs(user=ns.user, token=ns.token)
    return PublicArgs(user=ns.user, no_cache=ns.no_cache)


if __name__ == "__main__":
    try:
        asyncio.run(main(parse_args()))
    except KeyboardInterrupt:
        print()
